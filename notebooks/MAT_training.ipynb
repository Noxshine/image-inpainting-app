{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tải bộ dữ liệu"
      ],
      "metadata": {
        "id": "6Oy9pIZIfmjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        "# upload file kaggle.json\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "OrNeZ8e659R2",
        "outputId": "de3575a3-db4b-43ac-de3b-e10adcd7b0d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a2b02310-b1ae-4cc1-9647-ab00f0c46dc5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a2b02310-b1ae-4cc1-9647-ab00f0c46dc5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"tanguyen2112\",\"key\":\"86777dce6a1b7a946b11372c4efbb69b\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d vincenttamml/celebamaskhq512\n",
        "!unzip celebamaskhq512"
      ],
      "metadata": {
        "id": "1VrXRUo46AmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Đường dẫn tới thư mục chứa ảnh\n",
        "source_folder = \"/content/image\"  # Thay bằng đường dẫn thư mục nguồn\n",
        "output_folder = \"/content/image_divice\"  # Thay bằng đường dẫn thư mục đích\n",
        "\n",
        "# Tạo 10 thư mục đích nếu chưa tồn tại\n",
        "num_folders = 10\n",
        "for i in range(1, num_folders + 1):\n",
        "    os.makedirs(os.path.join(output_folder, f\"folder_{i}\"), exist_ok=True)\n",
        "\n",
        "# Lấy danh sách các tệp trong thư mục nguồn\n",
        "files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
        "\n",
        "# Phân phối tệp ngẫu nhiên vào 10 thư mục\n",
        "for file in files:\n",
        "    target_folder = os.path.join(output_folder, f\"folder_{random.randint(1, num_folders)}\")\n",
        "    shutil.move(os.path.join(source_folder, file), os.path.join(target_folder, file))\n",
        "\n",
        "print(\"Ảnh đã được chia vào 10 thư mục!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15nnHmI2ZQx6",
        "outputId": "1ba33fbd-8fa5-4363-8ad0-12e2a5011fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ảnh đã được chia vào 10 thư mục!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize tất cả các ảnh trong thư mục về kích thước 512x512\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "input_folder = '/content/image_divice/folder_2'  # Thay đổi nếu cần\n",
        "output_folder = '/content/image_resize_512_2'\n",
        "\n",
        "# Tạo thư mục output nếu chưa có\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Duyệt qua tất cả các ảnh trong thư mục và resize\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(('.png', '.jpg', '.jpeg')):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        image = Image.open(image_path)\n",
        "        image = image.resize((512, 512), Image.Resampling.LANCZOS)\n",
        "        image.save(os.path.join(output_folder, filename))\n",
        "\n",
        "print(\"Resize hoàn tất!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2nLfjnSWdcN",
        "outputId": "05ae86c0-8603-406d-974e-5336415b3a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resize hoàn tất!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chuẩn bị môi trường"
      ],
      "metadata": {
        "id": "KD1hQJ5Xw5yc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-Wwhcugqw8p"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/fenglinglwb/MAT.git\n",
        "\n",
        "%cd MAT\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chuẩn bị dữ liệu"
      ],
      "metadata": {
        "id": "b3r9FRvW2-BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randint, seed\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class CustomMaskGenerator1():\n",
        "    def __init__(self, height=512, width=512, num_lines=20, num_circles=20, num_elips=20, rand_seed=None, hole_range=[0, 1]):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = 1  # Output should be single-channel (grayscale)\n",
        "        self.num_lines = num_lines\n",
        "        self.num_circles = num_circles\n",
        "        self.num_elips = num_elips\n",
        "        self.hole_range = hole_range  # Control hole ratio in the mask\n",
        "\n",
        "        if rand_seed:\n",
        "            seed(rand_seed)\n",
        "\n",
        "    def _generate_mask(self):\n",
        "        \"\"\"Generates a random irregular mask with lines, circles, and ellipses\"\"\"\n",
        "        img = np.zeros((self.height, self.width), np.uint8)  # Single channel binary image\n",
        "\n",
        "        # Set size scale for the shapes\n",
        "        size = int((self.width + self.height) * 0.03)\n",
        "\n",
        "        # Draw random lines\n",
        "        for _ in range(randint(1, self.num_lines)):\n",
        "            x1, x2 = randint(1, self.width), randint(1, self.width)\n",
        "            y1, y2 = randint(1, self.height), randint(1, self.height)\n",
        "            thickness = randint(3, size)\n",
        "            cv2.line(img, (x1, y1), (x2, y2), 255, thickness)\n",
        "\n",
        "        # Draw random circles\n",
        "        for _ in range(randint(1, self.num_circles)):\n",
        "            x1, y1 = randint(1, self.width), randint(1, self.height)\n",
        "            radius = randint(3, size)\n",
        "            cv2.circle(img, (x1, y1), radius, 255, -1)\n",
        "\n",
        "        # Draw random ellipses\n",
        "        for _ in range(randint(1, self.num_elips)):\n",
        "            x1, y1 = randint(1, self.width), randint(1, self.height)\n",
        "            s1, s2 = randint(1, self.width), randint(1, self.height)\n",
        "            a1, a2, a3 = randint(3, 180), randint(3, 180), randint(3, 180)\n",
        "            thickness = randint(3, size)\n",
        "            cv2.ellipse(img, (x1, y1), (s1, s2), a1, a2, a3, 255, thickness)\n",
        "\n",
        "        # Now ensure that the mask has a proper hole ratio within the specified range\n",
        "        hole_ratio = 1 - np.mean(img / 255)  # Calculate hole ratio (1 means fully filled, 0 means all holes)\n",
        "\n",
        "        # If hole ratio is out of bounds, adjust the mask to have a valid hole ratio\n",
        "        while hole_ratio < self.hole_range[0] or hole_ratio > self.hole_range[1]:\n",
        "            # Generate a new mask if it does not satisfy hole ratio condition\n",
        "            img.fill(0)  # Reset the mask\n",
        "            # Re-draw the shapes\n",
        "            for _ in range(randint(1, self.num_lines)):\n",
        "                x1, x2 = randint(1, self.width), randint(1, self.width)\n",
        "                y1, y2 = randint(1, self.height), randint(1, self.height)\n",
        "                thickness = randint(3, size)\n",
        "                cv2.line(img, (x1, y1), (x2, y2), 255, thickness)\n",
        "\n",
        "            for _ in range(randint(1, self.num_circles)):\n",
        "                x1, y1 = randint(1, self.width), randint(1, self.height)\n",
        "                radius = randint(3, size)\n",
        "                cv2.circle(img, (x1, y1), radius, 255, -1)\n",
        "\n",
        "            for _ in range(randint(1, self.num_elips)):\n",
        "                x1, y1 = randint(1, self.width), randint(1, self.height)\n",
        "                s1, s2 = randint(1, self.width), randint(1, self.height)\n",
        "                a1, a2, a3 = randint(3, 180), randint(3, 180), randint(3, 180)\n",
        "                thickness = randint(3, size)\n",
        "                cv2.ellipse(img, (x1, y1), (s1, s2), a1, a2, a3, 255, thickness)\n",
        "\n",
        "            hole_ratio = 1 - np.mean(img / 255)  # Recalculate hole ratio\n",
        "\n",
        "        return img.astype(np.float32)[np.newaxis, ...]\n"
      ],
      "metadata": {
        "id": "XtmingQTK4rS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import PIL.Image\n",
        "import json\n",
        "import torch\n",
        "import dnnlib\n",
        "import random\n",
        "from random import randint, seed\n",
        "\n",
        "try:\n",
        "    import pyspng\n",
        "except ImportError:\n",
        "    pyspng = None\n",
        "\n",
        "from MAT.datasets.dataset_512 import Dataset\n",
        "\n",
        "gen_mask = CustomMaskGenerator1(height=512, width=512, num_lines=20, num_circles=20, num_elips=20, rand_seed=None, hole_range=[0, 1])\n",
        "\n",
        "\n",
        "class ImageFolderMaskDataset(Dataset):\n",
        "    def __init__(self,\n",
        "        path,                   # Path to directory or zip.\n",
        "        resolution      = None, # Ensure specific resolution, None = highest available.\n",
        "        hole_range=[0,1],\n",
        "        **super_kwargs,         # Additional arguments for the Dataset base class.\n",
        "    ):\n",
        "        self._path = path\n",
        "        self._zipfile = None\n",
        "        self._hole_range = hole_range\n",
        "\n",
        "        if os.path.isdir(self._path):\n",
        "            self._type = 'dir'\n",
        "            self._all_fnames = {os.path.relpath(os.path.join(root, fname), start=self._path) for root, _dirs, files in os.walk(self._path) for fname in files}\n",
        "        elif self._file_ext(self._path) == '.zip':\n",
        "            self._type = 'zip'\n",
        "            self._all_fnames = set(self._get_zipfile().namelist())\n",
        "        else:\n",
        "            raise IOError('Path must point to a directory or zip')\n",
        "\n",
        "        PIL.Image.init()\n",
        "        self._image_fnames = sorted(fname for fname in self._all_fnames if self._file_ext(fname) in PIL.Image.EXTENSION)\n",
        "        if len(self._image_fnames) == 0:\n",
        "            raise IOError('No image files found in the specified path')\n",
        "\n",
        "        name = os.path.splitext(os.path.basename(self._path))[0]\n",
        "        raw_shape = [len(self._image_fnames)] + list(self._load_raw_image(0).shape)\n",
        "        if resolution is not None and (raw_shape[2] != resolution or raw_shape[3] != resolution):\n",
        "            raise IOError('Image files do not match the specified resolution')\n",
        "        super().__init__(name=name, raw_shape=raw_shape, **super_kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def _file_ext(fname):\n",
        "        return os.path.splitext(fname)[1].lower()\n",
        "\n",
        "    def _get_zipfile(self):\n",
        "        assert self._type == 'zip'\n",
        "        if self._zipfile is None:\n",
        "            self._zipfile = zipfile.ZipFile(self._path)\n",
        "        return self._zipfile\n",
        "\n",
        "    def _open_file(self, fname):\n",
        "        if self._type == 'dir':\n",
        "            return open(os.path.join(self._path, fname), 'rb')\n",
        "        if self._type == 'zip':\n",
        "            return self._get_zipfile().open(fname, 'r')\n",
        "        return None\n",
        "\n",
        "    def close(self):\n",
        "        try:\n",
        "            if self._zipfile is not None:\n",
        "                self._zipfile.close()\n",
        "        finally:\n",
        "            self._zipfile = None\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return dict(super().__getstate__(), _zipfile=None)\n",
        "\n",
        "    def _load_raw_image(self, raw_idx):\n",
        "        fname = self._image_fnames[raw_idx]\n",
        "        with self._open_file(fname) as f:\n",
        "            if pyspng is not None and self._file_ext(fname) == '.png':\n",
        "                image = pyspng.load(f.read())\n",
        "            else:\n",
        "                image = np.array(PIL.Image.open(f))\n",
        "        if image.ndim == 2:\n",
        "            image = image[:, :, np.newaxis] # HW => HWC\n",
        "\n",
        "        # for grayscale image\n",
        "        if image.shape[2] == 1:\n",
        "            image = np.repeat(image, 3, axis=2)\n",
        "\n",
        "        # restricted to 512x512\n",
        "        res = 512\n",
        "        H, W, C = image.shape\n",
        "        if H < res or W < res:\n",
        "            top = 0\n",
        "            bottom = max(0, res - H)\n",
        "            left = 0\n",
        "            right = max(0, res - W)\n",
        "            image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_REFLECT)\n",
        "        H, W, C = image.shape\n",
        "        h = random.randint(0, H - res)\n",
        "        w = random.randint(0, W - res)\n",
        "        image = image[h:h+res, w:w+res, :]\n",
        "\n",
        "        image = np.ascontiguousarray(image.transpose(2, 0, 1)) # HWC => CHW\n",
        "\n",
        "        return image\n",
        "\n",
        "    def _load_raw_labels(self):\n",
        "        fname = 'labels.json'\n",
        "        if fname not in self._all_fnames:\n",
        "            return None\n",
        "        with self._open_file(fname) as f:\n",
        "            labels = json.load(f)['labels']\n",
        "        if labels is None:\n",
        "            return None\n",
        "        labels = dict(labels)\n",
        "        labels = [labels[fname.replace('\\\\', '/')] for fname in self._image_fnames]\n",
        "        labels = np.array(labels)\n",
        "        labels = labels.astype({1: np.int64, 2: np.float32}[labels.ndim])\n",
        "        return labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self._load_raw_image(self._raw_idx[idx])\n",
        "\n",
        "        assert isinstance(image, np.ndarray)\n",
        "        assert list(image.shape) == self.image_shape\n",
        "        assert image.dtype == np.uint8\n",
        "        if self._xflip[idx]:\n",
        "            assert image.ndim == 3 # CHW\n",
        "            image = image[:, :, ::-1]\n",
        "        mask = gen_mask._generate_mask()  # hole as 0, reserved as 1\n",
        "        return image.copy(), mask, self.get_label(idx)\n"
      ],
      "metadata": {
        "id": "o1v3x2LT3CSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_utils import misc\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import legacy\n",
        "\n",
        "from networks.mat import Generator, Discriminator\n",
        "from losses.loss import TwoStageLoss  # Import lớp TwoStageLoss\n",
        "\n",
        "start_time = time.time()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "dataset_path = '/content/image_resize_512'\n",
        "image_resolution=512\n",
        "batch_size = 2\n",
        "\n",
        "# Load dataset\n",
        "print('Loading dataset...')\n",
        "training_set = ImageFolderMaskDataset(path=dataset_path, resolution=image_resolution, hole_range=[0, 1])\n",
        "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "print(f'Dataset size: {len(training_set)} images')\n",
        "\n",
        "c_dim = training_set.label_dim\n",
        "img_channels = training_set.num_channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha-_Rq2WS2du",
        "outputId": "23e260e2-1ae1-429e-cbbb-ffd5dcb23857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset size: 2960 images\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/image_resize_512_2'\n",
        "image_resolution=512\n",
        "batch_size = 2\n",
        "\n",
        "# Load dataset\n",
        "print('Loading dataset...')\n",
        "training_set = ImageFolderMaskDataset(path=dataset_path, resolution=image_resolution, hole_range=[0, 1])\n",
        "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "print(f'Dataset size: {len(training_set)} images')\n",
        "\n",
        "c_dim = training_set.label_dim\n",
        "img_channels = training_set.num_channels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVILKiLGyRYm",
        "outputId": "07a71aef-84b6-4a18-d2d0-72d6eeb33c63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Dataset size: 2963 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine turn\n"
      ],
      "metadata": {
        "id": "K26qJpuJwz__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resume_specs = {\n",
        "    'ffhq256':     'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res256-mirror-paper256-noaug.pkl',\n",
        "    'ffhq512':     'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res512-mirror-stylegan2-noaug.pkl',\n",
        "    'ffhq1024':    'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res1024-mirror-stylegan2-noaug.pkl',\n",
        "    'celebahq256': 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/celebahq-res256-mirror-paper256-kimg100000-ada-target0.5.pkl',\n",
        "    'lsundog256':  'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/lsundog-res256-paper256-kimg100000-noaug.pkl',\n",
        "}\n",
        "\n",
        "# chọn mô hình muốn fine-turn\n",
        "model_path = resume_specs['ffhq512']\n",
        "\n",
        "pr=0.1\n",
        "pl=False\n",
        "gamma=5.0\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "betas=[0, 0.99]\n",
        "lr=0.0001"
      ],
      "metadata": {
        "id": "p0a7t1CG67_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_utils.misc import named_params_and_buffers\n",
        "\n",
        "def copy_params_and_buffers(src_module, dst_module, require_all=False):\n",
        "    assert isinstance(src_module, torch.nn.Module)\n",
        "    assert isinstance(dst_module, torch.nn.Module)\n",
        "    src_tensors = {name: tensor for name, tensor in named_params_and_buffers(src_module)}\n",
        "    for name, tensor in named_params_and_buffers(dst_module):\n",
        "        assert (name in src_tensors) or (not require_all)\n",
        "        if name in src_tensors:\n",
        "            tensor.data = src_tensors[name].detach().clone()\n",
        "            tensor.requires_grad_(tensor.requires_grad)"
      ],
      "metadata": {
        "id": "GTUTsyIscZLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(c_dim,image_resolution, img_channels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ojwGLqalpFD",
        "outputId": "5be163e5-8916-43b1-b8cc-acda4297c432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 512 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igcF-WTzFjsX",
        "outputId": "32852d59-e23f-4293-962b-d7c4c626ee38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "# Initialize networks\n",
        "G = Generator(z_dim=512, c_dim=c_dim, w_dim=512, img_resolution=image_resolution, img_channels=img_channels)\n",
        "D = Discriminator(c_dim=c_dim,img_resolution=image_resolution, img_channels=img_channels).to(device)\n",
        "G_ema = copy.deepcopy(G).eval()\n",
        "\n",
        "# Initialize loss\n",
        "loss_instance = TwoStageLoss(\n",
        "    device=device,\n",
        "    G_mapping=G.mapping,\n",
        "    G_synthesis=G.synthesis,\n",
        "    D=D,\n",
        "    r1_gamma=gamma if gamma is not None else 10.0,\n",
        "    pl_weight=0 if not pl else 1.0,\n",
        "    pcp_ratio=pr if pr is not None else 0.0\n",
        ")\n",
        "\n",
        "# Loading model\n",
        "print(f'Loading pretrained model from \"{model_path}\"')\n",
        "with dnnlib.util.open_url(model_path) as f:\n",
        "    resume_data = legacy.load_network_pkl(f)\n",
        "    for name, module in [('G', G), ('D', D), ('G_ema', G_ema)]:\n",
        "        copy_params_and_buffers(resume_data[name], module, require_all=False)\n",
        "        module.to(device)\n",
        "\n",
        "# Optimizers\n",
        "G_opt = torch.optim.Adam(G.parameters(), lr=lr, betas=(betas[0], betas[1]))\n",
        "D_opt = torch.optim.Adam(D.parameters(), lr=lr, betas=(betas[0], betas[1]))\n",
        "\n",
        "# Define the number of epochs\n",
        "num_epochs = 1\n",
        "\n",
        "checkpoint_interval = 1  # Define how often to save a checkpoint\n",
        "\n",
        "def training_loop(\n",
        "    training_loader,\n",
        "    D_opt,\n",
        "    G_opt,\n",
        "    loss_instance,\n",
        "    G,\n",
        "    G_ema\n",
        "):\n",
        "    i = 1\n",
        "\n",
        "    for  real_imgs, masks, labels in training_loader:\n",
        "        print(i)\n",
        "        i = i + 1\n",
        "        real_imgs = real_imgs.to(device).float() / 127.5 - 1  # Normalize to [-1, 1]\n",
        "        masks = masks.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Generate latent vectors and conditions\n",
        "        gen_z = torch.randn([batch_size, G.z_dim], device=device)\n",
        "        gen_c = labels\n",
        "\n",
        "        # Train Discriminator\n",
        "        D_opt.zero_grad()\n",
        "        loss_instance.accumulate_gradients(\n",
        "            phase='Dmain',\n",
        "            real_img=real_imgs,\n",
        "            mask=masks,\n",
        "            real_c=labels,\n",
        "            gen_z=gen_z,\n",
        "            gen_c=gen_c,\n",
        "            sync=True,\n",
        "            gain=1.0\n",
        "        )\n",
        "        D_opt.step()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        # Train Generator\n",
        "        G_opt.zero_grad()\n",
        "        loss_instance.accumulate_gradients(\n",
        "            phase='Gmain',\n",
        "            real_img=real_imgs,\n",
        "            mask=masks,\n",
        "            real_c=labels,\n",
        "            gen_z=gen_z,\n",
        "            gen_c=gen_c,\n",
        "            sync=True,\n",
        "            gain=1.0\n",
        "        )\n",
        "        G_opt.step()\n",
        "\n",
        "\n",
        "        # Update EMA for G\n",
        "        ema_kimg = 10\n",
        "        ema_beta = 0.5 ** (batch_size / (ema_kimg * 1000))\n",
        "        with torch.no_grad():\n",
        "          for p_ema, p in zip(G_ema.parameters(), G.parameters()):\n",
        "            p_ema.data = p.lerp(p_ema, ema_beta).data\n",
        "\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Starting epoch {epoch + 1}...')\n",
        "    gc.collect()\n",
        "\n",
        "    training_loop(\n",
        "        training_loader,\n",
        "        D_opt,\n",
        "        G_opt,\n",
        "        loss_instance,\n",
        "        G,\n",
        "        G_ema\n",
        "    )\n",
        "\n",
        "    print(f'Epoch {epoch + 1} completed.')\n",
        "\n",
        "    # Save checkpoints\n",
        "    if (epoch + 1) % checkpoint_interval == 0:\n",
        "        checkpoint_path = f'checkpoint_epoch_{epoch + 1}.pt'\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'G_state_dict': G.state_dict(),\n",
        "            'D_state_dict': D.state_dict(),\n",
        "            'G_ema_state_dict': G_ema.state_dict(),\n",
        "            'G_opt_state_dict': G_opt.state_dict(),\n",
        "            'D_opt_state_dict': D_opt.state_dict()\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "print('Training completed.')"
      ],
      "metadata": {
        "id": "x2Rp1ZYtw4zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9307380a-45e0-470e-9104-f073b91e1d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:09<00:00, 62.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained model from \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res512-mirror-stylegan2-noaug.pkl\"\n",
            "Downloading https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/transfer-learning-source-nets/ffhq-res512-mirror-stylegan2-noaug.pkl ... done\n",
            "Starting epoch 1...\n",
            "1\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n",
            "1128\n",
            "1129\n",
            "1130\n",
            "1131\n",
            "1132\n",
            "1133\n",
            "1134\n",
            "1135\n",
            "1136\n",
            "1137\n",
            "1138\n",
            "1139\n",
            "1140\n",
            "1141\n",
            "1142\n",
            "1143\n",
            "1144\n",
            "1145\n",
            "1146\n",
            "1147\n",
            "1148\n",
            "1149\n",
            "1150\n",
            "1151\n",
            "1152\n",
            "1153\n",
            "1154\n",
            "1155\n",
            "1156\n",
            "1157\n",
            "1158\n",
            "1159\n",
            "1160\n",
            "1161\n",
            "1162\n",
            "1163\n",
            "1164\n",
            "1165\n",
            "1166\n",
            "1167\n",
            "1168\n",
            "1169\n",
            "1170\n",
            "1171\n",
            "1172\n",
            "1173\n",
            "1174\n",
            "1175\n",
            "1176\n",
            "1177\n",
            "1178\n",
            "1179\n",
            "1180\n",
            "1181\n",
            "1182\n",
            "1183\n",
            "1184\n",
            "1185\n",
            "1186\n",
            "1187\n",
            "1188\n",
            "1189\n",
            "1190\n",
            "1191\n",
            "1192\n",
            "1193\n",
            "1194\n",
            "1195\n",
            "1196\n",
            "1197\n",
            "1198\n",
            "1199\n",
            "1200\n",
            "1201\n",
            "1202\n",
            "1203\n",
            "1204\n",
            "1205\n",
            "1206\n",
            "1207\n",
            "1208\n",
            "1209\n",
            "1210\n",
            "1211\n",
            "1212\n",
            "1213\n",
            "1214\n",
            "1215\n",
            "1216\n",
            "1217\n",
            "1218\n",
            "1219\n",
            "1220\n",
            "1221\n",
            "1222\n",
            "1223\n",
            "1224\n",
            "1225\n",
            "1226\n",
            "1227\n",
            "1228\n",
            "1229\n",
            "1230\n",
            "1231\n",
            "1232\n",
            "1233\n",
            "1234\n",
            "1235\n",
            "1236\n",
            "1237\n",
            "1238\n",
            "1239\n",
            "1240\n",
            "1241\n",
            "1242\n",
            "1243\n",
            "1244\n",
            "1245\n",
            "1246\n",
            "1247\n",
            "1248\n",
            "1249\n",
            "1250\n",
            "1251\n",
            "1252\n",
            "1253\n",
            "1254\n",
            "1255\n",
            "1256\n",
            "1257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'checkpoint_epoch_{epoch + 1}.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvdMDIDWzJqk",
        "outputId": "9a317538-785a-49da-baba-379bce97d71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint_epoch_1.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f'Starting epoch {epoch + 1}...')\n",
        "    gc.collect()\n",
        "\n",
        "    training_loop(\n",
        "        training_loader,\n",
        "        D_opt,\n",
        "        G_opt,\n",
        "        loss_instance,\n",
        "        G,\n",
        "        G_ema\n",
        "    )\n",
        "\n",
        "    print(f'Epoch {epoch + 1} completed.')\n",
        "\n",
        "    # Save checkpoints\n",
        "    if (epoch + 1) % checkpoint_interval == 0:\n",
        "        checkpoint_path = f'/content/checkpoint_epoch_{epoch + 1}.pt'\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'G_state_dict': G.state_dict(),\n",
        "            'D_state_dict': D.state_dict(),\n",
        "            'G_ema_state_dict': G_ema.state_dict(),\n",
        "            'G_opt_state_dict': G_opt.state_dict(),\n",
        "            'D_opt_state_dict': D_opt.state_dict()\n",
        "        }, checkpoint_path)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "print('Training completed.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCEA_JR3ycdt",
        "outputId": "304dce1e-d353-4a9d-e805-f3be7e62b792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n",
            "1000\n",
            "1001\n",
            "1002\n",
            "1003\n",
            "1004\n",
            "1005\n",
            "1006\n",
            "1007\n",
            "1008\n",
            "1009\n",
            "1010\n",
            "1011\n",
            "1012\n",
            "1013\n",
            "1014\n",
            "1015\n",
            "1016\n",
            "1017\n",
            "1018\n",
            "1019\n",
            "1020\n",
            "1021\n",
            "1022\n",
            "1023\n",
            "1024\n",
            "1025\n",
            "1026\n",
            "1027\n",
            "1028\n",
            "1029\n",
            "1030\n",
            "1031\n",
            "1032\n",
            "1033\n",
            "1034\n",
            "1035\n",
            "1036\n",
            "1037\n",
            "1038\n",
            "1039\n",
            "1040\n",
            "1041\n",
            "1042\n",
            "1043\n",
            "1044\n",
            "1045\n",
            "1046\n",
            "1047\n",
            "1048\n",
            "1049\n",
            "1050\n",
            "1051\n",
            "1052\n",
            "1053\n",
            "1054\n",
            "1055\n",
            "1056\n",
            "1057\n",
            "1058\n",
            "1059\n",
            "1060\n",
            "1061\n",
            "1062\n",
            "1063\n",
            "1064\n",
            "1065\n",
            "1066\n",
            "1067\n",
            "1068\n",
            "1069\n",
            "1070\n",
            "1071\n",
            "1072\n",
            "1073\n",
            "1074\n",
            "1075\n",
            "1076\n",
            "1077\n",
            "1078\n",
            "1079\n",
            "1080\n",
            "1081\n",
            "1082\n",
            "1083\n",
            "1084\n",
            "1085\n",
            "1086\n",
            "1087\n",
            "1088\n",
            "1089\n",
            "1090\n",
            "1091\n",
            "1092\n",
            "1093\n",
            "1094\n",
            "1095\n",
            "1096\n",
            "1097\n",
            "1098\n",
            "1099\n",
            "1100\n",
            "1101\n",
            "1102\n",
            "1103\n",
            "1104\n",
            "1105\n",
            "1106\n",
            "1107\n",
            "1108\n",
            "1109\n",
            "1110\n",
            "1111\n",
            "1112\n",
            "1113\n",
            "1114\n",
            "1115\n",
            "1116\n",
            "1117\n",
            "1118\n",
            "1119\n",
            "1120\n",
            "1121\n",
            "1122\n",
            "1123\n",
            "1124\n",
            "1125\n",
            "1126\n",
            "1127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Sau khi kết thúc huấn luyện, bạn lưu mô hình vào Google Drive\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/checkpoint_epoch_final.pt'\n",
        "\n",
        "torch.save({\n",
        "    'epoch': num_epochs - 1,  # Lưu epoch cuối cùng\n",
        "    'G_state_dict': G.state_dict(),\n",
        "    'D_state_dict': D.state_dict(),\n",
        "    'G_ema_state_dict': G_ema.state_dict(),\n",
        "    'G_opt_state_dict': G_opt.state_dict(),\n",
        "    'D_opt_state_dict': D_opt.state_dict()\n",
        "}, checkpoint_path)\n",
        "\n",
        "print(f'Checkpoint đã được lưu tại {checkpoint_path}')\n"
      ],
      "metadata": {
        "id": "kR2XAtOt9beZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jks4ywGuBnY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Đánh giá FID"
      ],
      "metadata": {
        "id": "Lhirz3fZzjdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Đường dẫn tới thư mục chứa ảnh\n",
        "source_folder = \"/content/image_divice/folder_5\"  # Thay bằng đường dẫn thư mục đích\n",
        "origin_folder = \"/content/drive/dataset/MAT/origin_image\"  # Thay bằng đường dẫn thư mục đích\n",
        "mask_folder = \"/content/drive/dataset/MAT/mask_image\"  # Thay bằng đường dẫn thư mục đích\n",
        "\n",
        "\n",
        "gen_mask = CustomMaskGenerator1(height=512, width=512, num_lines=20, num_circles=20, num_elips=20, rand_seed=None, hole_range=[0, 1])\n",
        "\n",
        "\n",
        "os.makedirs(os.path.join(origin_folder), exist_ok=True)\n",
        "os.makedirs(os.path.join(mask_folder), exist_ok=True)\n",
        "# Lấy danh sách các tệp trong thư mục nguồn\n",
        "files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
        "\n",
        "# Phân phối tệp ngẫu nhiên vào 10 thư mục\n",
        "for file in files:\n",
        "    shutil.move(os.path.join(source_folder, file), os.path.join(origin_folder, file))\n",
        "    mask = gen_mask._generate_mask()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Ảnh đã được chia vào 10 thư mục!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uo7PPIWE-SvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, '../')\n",
        "import numpy as np\n",
        "import math\n",
        "import glob\n",
        "import pyspng\n",
        "import PIL.Image\n",
        "import torch\n",
        "import dnnlib\n",
        "import scipy.linalg\n",
        "import sklearn.svm\n",
        "\n",
        "\n",
        "_feature_detector_cache = dict()\n",
        "\n",
        "def get_feature_detector(url, device=torch.device('cpu'), num_gpus=1, rank=0, verbose=False):\n",
        "    assert 0 <= rank < num_gpus\n",
        "    key = (url, device)\n",
        "    if key not in _feature_detector_cache:\n",
        "        is_leader = (rank == 0)\n",
        "        if not is_leader and num_gpus > 1:\n",
        "            torch.distributed.barrier() # leader goes first\n",
        "        with dnnlib.util.open_url(url, verbose=(verbose and is_leader)) as f:\n",
        "            _feature_detector_cache[key] = torch.jit.load(f).eval().to(device)\n",
        "        if is_leader and num_gpus > 1:\n",
        "            torch.distributed.barrier() # others follow\n",
        "    return _feature_detector_cache[key]\n",
        "\n",
        "\n",
        "def read_image(image_path):\n",
        "    with open(image_path, 'rb') as f:\n",
        "        if pyspng is not None and image_path.endswith('.png'):\n",
        "            image = pyspng.load(f.read())\n",
        "        else:\n",
        "            image = np.array(PIL.Image.open(f))\n",
        "    if image.ndim == 2:\n",
        "        image = image[:, :, np.newaxis] # HW => HWC\n",
        "    if image.shape[2] == 1:\n",
        "        image = np.repeat(image, 3, axis=2)\n",
        "    image = image.transpose(2, 0, 1) # HWC => CHW\n",
        "    image = torch.from_numpy(image).unsqueeze(0).to(torch.uint8)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "class FeatureStats:\n",
        "    def __init__(self, capture_all=False, capture_mean_cov=False, max_items=None):\n",
        "        self.capture_all = capture_all\n",
        "        self.capture_mean_cov = capture_mean_cov\n",
        "        self.max_items = max_items\n",
        "        self.num_items = 0\n",
        "        self.num_features = None\n",
        "        self.all_features = None\n",
        "        self.raw_mean = None\n",
        "        self.raw_cov = None\n",
        "\n",
        "    def set_num_features(self, num_features):\n",
        "        if self.num_features is not None:\n",
        "            assert num_features == self.num_features\n",
        "        else:\n",
        "            self.num_features = num_features\n",
        "            self.all_features = []\n",
        "            self.raw_mean = np.zeros([num_features], dtype=np.float64)\n",
        "            self.raw_cov = np.zeros([num_features, num_features], dtype=np.float64)\n",
        "\n",
        "    def is_full(self):\n",
        "        return (self.max_items is not None) and (self.num_items >= self.max_items)\n",
        "\n",
        "    def append(self, x):\n",
        "        x = np.asarray(x, dtype=np.float32)\n",
        "        assert x.ndim == 2\n",
        "        if (self.max_items is not None) and (self.num_items + x.shape[0] > self.max_items):\n",
        "            if self.num_items >= self.max_items:\n",
        "                return\n",
        "            x = x[:self.max_items - self.num_items]\n",
        "\n",
        "        self.set_num_features(x.shape[1])\n",
        "        self.num_items += x.shape[0]\n",
        "        if self.capture_all:\n",
        "            self.all_features.append(x)\n",
        "        if self.capture_mean_cov:\n",
        "            x64 = x.astype(np.float64)\n",
        "            self.raw_mean += x64.sum(axis=0)\n",
        "            self.raw_cov += x64.T @ x64\n",
        "\n",
        "    def append_torch(self, x, num_gpus=1, rank=0):\n",
        "        assert isinstance(x, torch.Tensor) and x.ndim == 2\n",
        "        assert 0 <= rank < num_gpus\n",
        "        if num_gpus > 1:\n",
        "            ys = []\n",
        "            for src in range(num_gpus):\n",
        "                y = x.clone()\n",
        "                torch.distributed.broadcast(y, src=src)\n",
        "                ys.append(y)\n",
        "            x = torch.stack(ys, dim=1).flatten(0, 1) # interleave samples\n",
        "        self.append(x.cpu().numpy())\n",
        "\n",
        "    def get_all(self):\n",
        "        assert self.capture_all\n",
        "        return np.concatenate(self.all_features, axis=0)\n",
        "\n",
        "    def get_all_torch(self):\n",
        "        return torch.from_numpy(self.get_all())\n",
        "\n",
        "    def get_mean_cov(self):\n",
        "        assert self.capture_mean_cov\n",
        "        mean = self.raw_mean / self.num_items\n",
        "        cov = self.raw_cov / self.num_items\n",
        "        cov = cov - np.outer(mean, mean)\n",
        "        return mean, cov\n",
        "\n",
        "    def save(self, pkl_file):\n",
        "        with open(pkl_file, 'wb') as f:\n",
        "            pickle.dump(self.__dict__, f)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(pkl_file):\n",
        "        with open(pkl_file, 'rb') as f:\n",
        "            s = dnnlib.EasyDict(pickle.load(f))\n",
        "        obj = FeatureStats(capture_all=s.capture_all, max_items=s.max_items)\n",
        "        obj.__dict__.update(s)\n",
        "        return obj\n",
        "\n",
        "\n",
        "def calculate_metrics(folder1, folder2):\n",
        "    l1 = sorted(glob.glob(folder1 + '/*.png') + glob.glob(folder1 + '/*.jpg'))\n",
        "    l2 = sorted(glob.glob(folder2 + '/*.png') + glob.glob(folder2 + '/*.jpg'))\n",
        "    assert(len(l1) == len(l2))\n",
        "    print('length:', len(l1))\n",
        "\n",
        "    # l1 = l1[:3]; l2 = l2[:3];\n",
        "\n",
        "    # build detector\n",
        "    detector_url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/inception-2015-12-05.pt'\n",
        "    detector_kwargs = dict(return_features=True) # Return raw features before the softmax layer.\n",
        "    device = torch.device('cuda:0')\n",
        "    detector = get_feature_detector(url=detector_url, device=device, num_gpus=1, rank=0, verbose=False)\n",
        "    detector.eval()\n",
        "\n",
        "    stat1 = FeatureStats(capture_all=True, capture_mean_cov=True, max_items=len(l1))\n",
        "    stat2 = FeatureStats(capture_all=True, capture_mean_cov=True, max_items=len(l1))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (fpath1, fpath2) in enumerate(zip(l1, l2)):\n",
        "            print(i)\n",
        "            _, name1 = os.path.split(fpath1)\n",
        "            _, name2 = os.path.split(fpath2)\n",
        "            name1 = name1.split('.')[0]\n",
        "            name2 = name2.split('.')[0]\n",
        "            assert name1 == name2, 'Illegal mapping: %s, %s' % (name1, name2)\n",
        "\n",
        "            img1 = read_image(fpath1).to(device)\n",
        "            img2 = read_image(fpath2).to(device)\n",
        "            assert img1.shape == img2.shape, 'Illegal shape'\n",
        "            fea1 = detector(img1, **detector_kwargs)\n",
        "            stat1.append_torch(fea1, num_gpus=1, rank=0)\n",
        "            fea2 = detector(img2, **detector_kwargs)\n",
        "            stat2.append_torch(fea2, num_gpus=1, rank=0)\n",
        "\n",
        "    # calculate fid\n",
        "    mu1, sigma1 = stat1.get_mean_cov()\n",
        "    mu2, sigma2 = stat2.get_mean_cov()\n",
        "    m = np.square(mu1 - mu2).sum()\n",
        "    s, _ = scipy.linalg.sqrtm(np.dot(sigma1, sigma2), disp=False) # pylint: disable=no-member\n",
        "    fid = np.real(m + np.trace(sigma1 + sigma2 - s * 2))\n",
        "\n",
        "    return fid\n"
      ],
      "metadata": {
        "id": "zQcwzQHhzmsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    folder1 = 'path to the inpainted result'\n",
        "    folder2 = 'path to the gt'\n",
        "\n",
        "    fid = calculate_metrics(folder1, folder2)\n",
        "    print('fid: %.4f' % (fid))\n",
        "    with open('fid_pids_uids.txt', 'w') as f:\n",
        "        f.write('fid: %.4f' % (fid))"
      ],
      "metadata": {
        "id": "khsVPFBn8jXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}